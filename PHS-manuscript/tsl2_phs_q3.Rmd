---
title: "TSL2 Hypothesis 3"
author: "A.G. Mitchell"
date: '2022-06-07'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

##### Libraries #####
if (!require("pacman")) install.packages("pacman")
  pacman::p_load(boot, broom.mixed, caret, ggeffects, 
                 gghalves, ggpol, ggpubr, groupdata2, lme4, lmerTest, 
                 RColorBrewer, rcompanion,
                 reshape2, Rmisc, ROCR, rsample, tidyverse, wesanderson
                 )
```

# Getting data
```{r}
tsl_file <- file.path("data", "anonymised_tsl2_208.csv")
sum_file <- file.path("data", "summary_tsl2_208.csv")

# load all data file
if (file.exists(tsl_file)){
  tsl2 <- read.csv(tsl_file)
  } else {
    print('Data file does not exist, check data is in current directory.')
  }
# load data summary file
if (file.exists(sum_file)){
  sum_dat <- read.csv(sum_file)
  } else {
    print('Data file does not exist, check data is in current directory if not, run tsl2_phs_q1.Rmd first')
  }
```

Functions at the top
# Zscore function
```{r}
calc_z <- function (value) {
  z_score <- (value - mean(value))/sd(value)
}
```

# contrast function
Calculating contrast
TCF - thermal contrast function (developed by FF)
```{r}
# the TCF assumes that maximum possible TSL temperature is 50deg
TCF <- function (Tmax, Tmin) {
  top_temp = 50
  contrast <- (Tmax - Tmin)/top_temp
}
```

# Organise data and calculate contrast for each trial
# First innocuous thermal contrast
```{r}
# extract relevant trials (etc)
df <- tsl2 %>% 
  filter(task == "tsl2" & quality == 'cold' & trials_to_keep == 1) %>% 
  select(c(exp_id,instruction,trial,baseline,threshold,phs,age,gender)) %>% 
  dplyr::rename(subject = exp_id,
         phs_01 = phs)
# any value < 0 should = 0
df$threshold[df$threshold < 0] <- 0
  
# using the thermal contrast function:
  # (tmax - tmin)/max possible temp (50)
df_con <- df
baselineT <- 32 #temperature below which pp can detect change

# then calculate contrast from max temperature
df_con <- df_con %>% 
  mutate(TCF = TCF(baseline, threshold))

# seperate column that includes MCF (for comparison)
#df_con <- df_con %>% 
#  mutate(MCF = MCF(baseline, threshold))

# remove MCF (for now)
#df_con <- df_con %>% 
#  select(-MCF)
```

# Add trial 1-back to the data-frame
```{r}
# first get values from trial 2, from original tsl df
t2 <- tsl2 %>% 
  filter(task == "tsl2" & quality == 'cold' & trial == 2) %>% 
  select(c(exp_id,instruction,trial,baseline,threshold,phs,age,gender)) %>% 
  dplyr::rename(subject = exp_id,
         phs_01 = phs)
# any value < 0 should = 0
t2$threshold[t2$threshold < 0] <- 0

# calculate contrast in this data-frame and filter to detect only
t2 <- t2 %>% 
    mutate(TCF = TCF(baseline, threshold)) %>% 
    filter(instruction == 'detect')

# isolate innocuous trials
df_conN <- df_con %>% 
  filter(instruction == 'detect')

# finally, merge with contrast df
df_conN <- rbind(df_conN, t2)

# first remove participant 40 (trial 4 = trial 2, so below code will not work on them)
# isolate 40
id37 <- df_conN %>% 
  filter(subject == 37)
# the remove
df_conN <- df_conN %>% 
  filter(subject != 37)

# add nback trials to data-frame
# loop through participants
df_conN$subject <- as.factor(df_conN$subject)
df_conN$baseline <- as.factor(df_conN$baseline)

df_conN <- df_conN %>% 
  group_by(subject, baseline) %>% 
  dplyr::mutate(
    tcf_nback = case_when(
      trial == 3 ~ TCF[trial==2],
      trial == 4 ~ TCF[trial==3],
      trial == 5 ~ TCF[trial==4]
      #TRUE ~ as.character(NA)
      )
  ) %>% 
  # remove trials = 2
  filter(trial > 2)

# then do the same for expid 40, but just don't use trial 4
id37$subject <- as.factor(id37$subject)
id37$baseline <- as.factor(id37$baseline)
# baselines with complete data
id37_1 <- id37 %>% 
  filter(baseline != 38) %>% 
  group_by(baseline) %>% 
  dplyr::mutate(
    tcf_nback = case_when(
      trial == 3 ~ TCF[trial==2],
      trial == 4 ~ TCF[trial==3],
      trial == 5 ~ TCF[trial==4]
      #TRUE ~ as.character(NA)
      )
  ) %>% 
  # remove trials = 2
  filter(trial > 2)

# baseline 38 with incomplete data
# keep nback contrasts the same
id37_2 <- id37 %>% 
  filter(baseline == 38) %>% 
  group_by(baseline) %>% 
  dplyr::mutate(
    tcf_nback = case_when(
      trial == 3 ~ TCF[1],
      trial == 5 ~ TCF[trial==3]
      #TRUE ~ as.character(NA)
      )
  ) 

id37_2 <- id37_2[1:3 ,]

# merge id40 back into main frame
id37 <- rbind(id37_1, id37_2)
df_conN <- rbind(df_conN, id37)
# then remove NaN value
df_conN <- df_conN %>% 
  filter(!is.na(tcf_nback))


# Need to average contrast for pain across trials
# Then add them back into data-frame
```

# Add median pain thresholds and finalise model data-set
```{r}
# isolate noxious trials
df_nox <-  df_con %>% 
  filter(instruction == 'pain')
# average TCF across trials
nox_tcf <- aggregate(TCF~subject+baseline, median, data = df_nox)
# average threshold across trials
nox_thr <- aggregate(threshold~subject+baseline, median, data = df_nox)
nox <- merge(nox_tcf, nox_thr)

# combine the two data-frames
nox$subject <- as.factor(nox$subject)
nox$baseline <- as.factor(nox$baseline)

df_conN <- left_join(df_conN, nox, by = c('subject','baseline'))
df_conN <- df_conN %>% 
  dplyr::rename(TCFdetect = TCF.x,
         TCFpain = TCF.y,
         threshold_detect = threshold.x,
         threshold_pain = threshold.y) %>% 
  select(-instruction)

# transformaing variables
# z-score does not work for contrast, because it smooths out difference between conditions
# log-transform contrast and nback contrast, 
# this maintains differences betweeen baseline more clearly
df_conN <- df_conN %>% 
  mutate(log_TCFdetect = log(TCFdetect),
         log_nback = log(tcf_nback),
         log_TCFpain = log(TCFpain))

# save the file
write.csv(df_conN, file.path("data", "model_contrast_data.csv"), row.names = FALSE)
```

# Summary statistics for contrast
```{r}
N = 208
# calculate contrast from summary (easier than recalculating summary table)
con_cond <- sum_dat %>% 
  mutate(TCF = TCF(baseline, threshold))

# then get summary statistics, including PHS
con_mean <- aggregate(TCF~baseline*instruction*phs_01, mean, data = con_cond)
con_median <- aggregate(TCF~baseline*instruction*phs_01, median, data = con_cond)
con_sd <- aggregate(TCF~baseline*instruction*phs_01, sd, data = con_cond)
con_stderr <- con_sd$TCF/sqrt(N)# number of participants
con_ci <- aggregate(TCF~baseline*instruction*phs_01, CI, data = con_cond)
# data frame
con_stats <- data.frame(baseline = con_mean$baseline, instruction = con_mean$instruction, 
                         phs_01 = con_mean$phs_01, 
                         mean = con_mean$TCF, median = con_median$TCF, 
                         sd = con_sd$TCF, stderr = con_stderr, con_ci$TCF)
# rename CI labels
names(con_stats)[names(con_stats) == 'upper'] <- 'ci_upper'
names(con_stats)[names(con_stats) == 'mean.1'] <- 'ci_mean'
names(con_stats)[names(con_stats) == 'lower'] <- 'ci_lower'
# calculate ci
con_stats$ci <- con_stats$ci_upper - con_stats$ci_lower

# save summary statistics
write.csv(con_cond, file.path("data", "con_summary_208.csv"), row.names = FALSE)
write.csv(con_stats, file.path("results", "phs_contrast_stats.csv"), row.names = FALSE)
con_stats
```

# run logistic regression models
1: phs~baseline
2: phs~TCFdetect
3: phs~TCFpain
4: phs~TCFdetect * TCFpain
5: phs~TCFdetect * TCFnback
6: phs~TCFdetect * TCFpain * TCFnback

# Model 1: PHS~baseline
```{r}
mod.1 <- lme4::glmer(phs_01 ~ baseline + (1|subject),
                data = df_conN,
                family = 'binomial') #, weights = train_dp$weights)
summary(mod.1)

# calculating odds ratios
or.1 <- tidy(mod.1, conf.int=TRUE, 
               exponentiate=TRUE, effects="fixed")
or.1
# save model
saveRDS(mod.1, file.path('results','h3_mod1.RData'))
```
# Model 2: PHS~TCSdetect (log of)
```{r}
# just contrast model
mod.2 <- lme4::glmer(phs_01 ~ log_TCFdetect + (1|subject),
                data = df_conN,
                family = 'binomial') #, weights = train_dp$weights)

summary(mod.2)

# calculating odds ratios
or.2 <- tidy(mod.2, conf.int=TRUE, 
               exponentiate=TRUE, effects="fixed")
or.2
# save model
saveRDS(mod.2, file.path('results','h3_mod2.RData'))
```
# Model 3: phs~TCFpain (log of)
```{r}
mod.3 <- lme4::glmer(phs_01 ~ log_TCFpain + (1|subject),
                data = df_conN,
                family = 'binomial') #, weights = train_dp$weights)
summary(mod.3)

# calculating odds ratios
or.3 <- tidy(mod.3, conf.int=TRUE, 
               exponentiate=TRUE, effects="fixed")
or.3
# save model
saveRDS(mod.3, file.path('results','h3_mod3.RData'))
```

# Model 4: phs~TCFdetect * TCFpain (log of)
```{r}
mod.4 <- lme4::glmer(phs_01 ~ log_TCFdetect * log_TCFpain + (1|subject),
                data = df_conN,
                family = 'binomial') #, weights = train_dp$weights)
summary(mod.4)

# calculating odds ratios
or.4 <- tidy(mod.4, conf.int=TRUE, 
               exponentiate=TRUE, effects="fixed")
or.4
# save model
saveRDS(mod.4, file.path('results','h3_mod4.RData'))
```

# Model 5: phs~TCFdetect * nbackTCF (log of)
```{r}
mod.5 <- lme4::glmer(phs_01 ~ log_TCFdetect + log_nback + (1|subject),
                data = df_conN,
                family = 'binomial') #, weights = train_dp$weights)
summary(mod.5)

# calculating odds ratios
or.5 <- tidy(mod.5, conf.int=TRUE, 
               exponentiate=TRUE, effects="fixed")
or.5
# save model
saveRDS(mod.5, file.path('results','h3_mod5.RData'))
```

# Model 6: phs~TCFdetect * TCFpain + nbackTCF (log of)
```{r}
mod.6 <- lme4::glmer(phs_01 ~ log_TCFdetect * log_TCFpain + log_nback + (1|subject),
                data = df_conN,
                family = 'binomial') #, weights = train_dp$weights)
summary(mod.6)

# calculating odds ratios
or.6 <- tidy(mod.6, conf.int=TRUE, 
               exponentiate=TRUE, effects="fixed")
or.6
# save model
saveRDS(mod.6, file.path('results','h3_mod6.RData'))
```

# Model 7: phs~threshold_detect * threshold_pain
```{r}
# zscore them
df_conN$zthreshold_detect <- calc_z(df_conN$threshold_detect)
df_conN$zthreshold_pain <- calc_z(df_conN$threshold_pain)

mod.7 <- lme4::glmer(phs_01 ~ zthreshold_detect * zthreshold_pain + (1|subject),
                data = df_conN,
                family = 'binomial') #, weights = train_dp$weights)
summary(mod.7)

# calculating odds ratios
or.7 <- tidy(mod.7, conf.int=TRUE, 
               exponentiate=TRUE, effects="fixed")
or.7
# save model
saveRDS(mod.7, file.path('results','h3_mod7.RData'))
```

# Finally, run model 3D (full model, no n back) with age and gender
```{r}
mod.8 <- lme4::glmer(phs_01 ~ log_TCFdetect * log_TCFpain + age + as.factor(gender) + (1|subject),
                data = df_conN,
                family = 'binomial') #, weights = train_dp$weights)
summary(mod.8)

# calculating odds ratios
or.8 <- tidy(mod.8, conf.int=TRUE, 
               exponentiate=TRUE, effects="fixed")
or.8
# save model
saveRDS(mod.8, file.path('results','supp_mod8.RData'))
```


# Model comparisons
Comparing: models 1, 2, 3 and 4

```{r}
# compare models
AIC(mod.1, mod.2, mod.3, mod.4, mod.7)
anova(mod.1, mod.2, mod.3, mod.4, mod.7)

# model 1 and 4 significantly higher than 3 and 2
# compare these two together
anova(mod.1, mod.4)

# model 4 is the winner :D
```

And: models 2, 4, 5 and 6
```{r}
# compare models
AIC(mod.2, mod.4, mod.5, mod.6)
anova(mod.2, mod.4, mod.5, mod.6)

# models 5, 4 and 6 are significantly better than model 2
# compare the three
anova(mod.4, mod.5, mod.6)

# model 4 and 6 win over 5, compare these two
anova(mod.4, mod.6)
# model 6 is the winner
```
 
# Sensitivity analyses
# Compute ROC from data-based models
```{r}
#load get_ROCR_data function from file
source("get_ROCR_data.R")

# Model 1: phs~baseline
hist(predict(mod.1, re.form = ~0))
pred_m1 <- predict(mod.1, type="response", re.form = ~0)
hist(pred_m1)

mod.1_pred_obj <- prediction(predictions = pred_m1, labels = df_conN$phs_01)
mod.1_perf_roc <- performance(prediction.obj = mod.1_pred_obj, 
                                  measure = "tpr",
                                  x.measure = "fpr")
mod.1_perf_auc <- performance(prediction.obj = mod.1_pred_obj, 
                                  measure = "auc")
mod.1_perf <- cbind(get_ROCR_data(mod.1_perf_roc),
                        get_ROCR_data(mod.1_perf_auc))

mod.1_perf$model <- "baseline" #add model identification for plotting

# Model 2: phs~TCSdetect
hist(predict(mod.2, re.form = ~0))
pred_m2 <- predict(mod.2, type="response", re.form = ~0)
hist(pred_m2)

mod.2_pred_obj <- prediction(predictions = pred_m2, labels = df_conN$phs_01)
mod.2_perf_roc <- performance(prediction.obj = mod.2_pred_obj, 
                                  measure = "tpr",
                                  x.measure = "fpr")
mod.2_perf_auc <- performance(prediction.obj = mod.2_pred_obj, 
                                  measure = "auc")
mod.2_perf <- cbind(get_ROCR_data(mod.2_perf_roc),
                        get_ROCR_data(mod.2_perf_auc))

mod.2_perf$model <- "innocuous TCF" #add model identification for plotting

# Model 3: phs~TCSpain
hist(predict(mod.3, re.form = ~0))
pred_m3 <- predict(mod.3, type="response", re.form = ~0)
hist(pred_m3)

mod.3_pred_obj <- prediction(predictions = pred_m3, labels = df_conN$phs_01)
mod.3_perf_roc <- performance(prediction.obj = mod.3_pred_obj, 
                                  measure = "tpr",
                                  x.measure = "fpr")
mod.3_perf_auc <- performance(prediction.obj = mod.3_pred_obj, 
                                  measure = "auc")
mod.3_perf <- cbind(get_ROCR_data(mod.3_perf_roc),
                        get_ROCR_data(mod.3_perf_auc))

mod.3_perf$model <- "noxious TCF" #add model identification for plotting

# Model 4: phs~TCSdetect*TCSpain
hist(predict(mod.4, re.form = ~0))
pred_m4 <- predict(mod.4, type="response", re.form = ~0)
hist(pred_m4)

mod.4_pred_obj <- prediction(predictions = pred_m4, labels = df_conN$phs_01)
mod.4_perf_roc <- performance(prediction.obj = mod.4_pred_obj, 
                                  measure = "tpr",
                                  x.measure = "fpr")
mod.4_perf_auc <- performance(prediction.obj = mod.4_pred_obj, 
                                  measure = "auc")
mod.4_perf <- cbind(get_ROCR_data(mod.4_perf_roc),
                        get_ROCR_data(mod.4_perf_auc))

mod.4_perf$model <- "innocuous TCF \n* noxious TCF" #add model identification for plotting

# Model 5: phs~TCSdetect*nback
hist(predict(mod.5, re.form = ~0))
pred_m5 <- predict(mod.5, type="response", re.form = ~0)
hist(pred_m5)

mod.5_pred_obj <- prediction(predictions = pred_m5, labels = df_conN$phs_01)
mod.5_perf_roc <- performance(prediction.obj = mod.5_pred_obj, 
                                  measure = "tpr",
                                  x.measure = "fpr")
mod.5_perf_auc <- performance(prediction.obj = mod.5_pred_obj, 
                                  measure = "auc")
mod.5_perf <- cbind(get_ROCR_data(mod.5_perf_roc),
                        get_ROCR_data(mod.5_perf_auc))

mod.5_perf$model <- "innocuous TCF \n+ 1-back" #add model identification for plotting

# Model 6: phs~TCSdetect*TCSpain+nback
hist(predict(mod.6, re.form = ~0))
pred_m6 <- predict(mod.6, type="response", re.form = ~0)
hist(pred_m6)

mod.6_pred_obj <- prediction(predictions = pred_m6, labels = df_conN$phs_01)
mod.6_perf_roc <- performance(prediction.obj = mod.6_pred_obj, 
                                  measure = "tpr",
                                  x.measure = "fpr")
mod.6_perf_auc <- performance(prediction.obj = mod.6_pred_obj, 
                                  measure = "auc")
mod.6_perf <- cbind(get_ROCR_data(mod.6_perf_roc),
                        get_ROCR_data(mod.6_perf_auc))

mod.6_perf$model <- "innocuous TCF \n+ 1-back" #add model identification for plotting

# Model 7: TSL thresholds
hist(predict(mod.7, re.form = ~0))
pred_m7 <- predict(mod.7, type="response", re.form = ~0)
hist(pred_m7)

mod.7_pred_obj <- prediction(predictions = pred_m7, labels = df_conN$phs_01)
mod.7_perf_roc <- performance(prediction.obj = mod.7_pred_obj, 
                                  measure = "tpr",
                                  x.measure = "fpr")
mod.7_perf_auc <- performance(prediction.obj = mod.7_pred_obj, 
                                  measure = "auc")
mod.7_perf <- cbind(get_ROCR_data(mod.7_perf_roc),
                        get_ROCR_data(mod.7_perf_auc))

mod.7_perf$model <- "innocuous TSL \n* noxious TSL" #add model identification for plotting
```

# Save model ROC data-frames
```{r}
write.csv(mod.1_perf, file.path("results", "h3_model1_ROC.csv"), row.names = FALSE)
write.csv(mod.2_perf, file.path("results", "h3_model2_ROC.csv"), row.names = FALSE)
write.csv(mod.3_perf, file.path("results", "h3_model3_ROC.csv"), row.names = FALSE)
write.csv(mod.4_perf, file.path("results", "h3_model4_ROC.csv"), row.names = FALSE)
write.csv(mod.5_perf, file.path("results", "h3_model5_ROC.csv"), row.names = FALSE)
write.csv(mod.6_perf, file.path("results", "h3_model6_ROC.csv"), row.names = FALSE)
write.csv(mod.7_perf, file.path("results", "h3_model7_ROC.csv"), row.names = FALSE)
```

**READ ME FIRST**
If you do not wish to run the whole bootstrapping procedure (chunks 14 - 16), which can take up to 5 hours, change 'run_bootstrap' to 0, this will skip these chunks. The saved model .csv files can be found here: https://osf.io/t3n8s/ and can be used to create figure 3 in figure3_v1.Rmd

```{r}
run_bootstrap = 0
```

# Bootstrap both model fits (simple and complex) to get an idea of confidence of AUC
Use 20 replicates during code testing, 2000 for data analysis and model checks
This step can take a few hours so double check whether it is needed (ie. has already been run)

```{r}
if (isTRUE(run_bootstrap == 1)){
  #identify ids for bootstrap replicates
  B <- 20 #number of bootstrap replicates
  ids <- unique(df_conN$subject) #unique ids
  N <- length(ids) #sample size
  
  #create a list of per subject mini datasets to use for bs
  id_sets_list <- list() #list to store mini-datasets,m separete one for each subject, empty atm
  #populate id_sets_list
  for (id in ids) {
    id_sets_list[[id]] <- df_conN[df_conN$subject == id, ]
  }
  
  #create datasets for bootstrapping using rsample package
  bs_train <- list() #empty list to store datasets
  bs_test <- list() 
  set.seed(389) #so it is reproducible
  for (i in seq.int(B)) {
    bs_train[[i]] <- bind_rows(sample(id_sets_list, N, replace = TRUE))
    bs_test[[i]] <- bind_rows(sample(id_sets_list, N, replace = TRUE))
  }
}
```

# Function to get a dataset similar to df_con, fits a model and extracts data 
# from ROCR performance objects
# Should also work on a df list 

```{r}
#extracted data: tpr, fpr, auc
get_performance_df <- function (train, test) {
  # calculate zscores, this needs to be done seperately for testing and training sets
  train <- train %>% 
    mutate(log_TCFdetect = log(TCFdetect),
           log_nback = log(tcf_nback),
           log_TCFpain = log(TCFpain),
           zthreshold_detect = calc_z(threshold_detect),
           zthreshold_pain = calc_z(threshold_pain))
  # renormalise data - testing
  test <- test %>% 
    mutate(log_TCFdetect = log(TCFdetect),
           log_nback = log(tcf_nback),
           log_TCFpain = log(TCFpain),
           zthreshold_detect = calc_z(threshold_detect),
           zthreshold_pain = calc_z(threshold_pain))
 
  ## run models - each one in turn ##
  mod.1 <- lme4::glmer(phs_01 ~ baseline + (1|subject),
                data = train,
                family = 'binomial') #, weights = train_dp$weights)
  # make sure warnings get logged - remove simulation later
  warning <- as.character(summary(mod.1)$optinfo$conv$lme4$messages)
  if(length(warning) != 0) print(warning)
  warning <- ifelse(length(warning) == 0, NA, warning)
  
  mod.2 <- lme4::glmer(phs_01 ~ log_TCFdetect + (1|subject),
                data = train,
                family = 'binomial') #, weights = train_dp$weights)
   
  # make sure warnings get logged - remove simulation later
  warning <- as.character(summary(mod.2)$optinfo$conv$lme4$messages)
  if(length(warning) != 0) print(warning)
  warning <- ifelse(length(warning) == 0, NA, warning)
  
  mod.3 <- lme4::glmer(phs_01 ~ log_TCFpain + (1|subject),
                data = train,
                family = 'binomial') #, weights = train_dp$weights)
   
  # make sure warnings get logged - remove simulation later
  warning <- as.character(summary(mod.3)$optinfo$conv$lme4$messages)
  if(length(warning) != 0) print(warning)
  warning <- ifelse(length(warning) == 0, NA, warning)
  
  mod.4 <- lme4::glmer(phs_01 ~ log_TCFdetect * log_TCFpain + (1|subject),
                data = train,
                family = 'binomial') #, weights = train_dp$weights)
   
  # make sure warnings get logged - remove simulation later
  warning <- as.character(summary(mod.4)$optinfo$conv$lme4$messages)
  if(length(warning) != 0) print(warning)
  warning <- ifelse(length(warning) == 0, NA, warning)
  
  mod.5 <- lme4::glmer(phs_01 ~ log_TCFdetect + tcf_nback + (1|subject),
                data = train,
                family = 'binomial') #, weights = train_dp$weights)
   
  # make sure warnings get logged - remove simulation later
  warning <- as.character(summary(mod.5)$optinfo$conv$lme4$messages)
  if(length(warning) != 0) print(warning)
  warning <- ifelse(length(warning) == 0, NA, warning)
  
  mod.6 <- lme4::glmer(phs_01 ~ log_TCFdetect * log_TCFpain + tcf_nback + (1|subject),
                data = train,
                family = 'binomial') #, weights = train_dp$weights)
   
  # make sure warnings get logged - remove simulation later
  warning <- as.character(summary(mod.6)$optinfo$conv$lme4$messages)
  if(length(warning) != 0) print(warning)
  warning <- ifelse(length(warning) == 0, NA, warning)
  
  mod.7 <- lme4::glmer(phs_01 ~ threshold_detect * threshold_pain + (1|subject),
                data = train,
                family = 'binomial') #, weights = train_dp$weights)
   
  # make sure warnings get logged - remove simulation later
  warning <- as.character(summary(mod.7)$optinfo$conv$lme4$messages)
  if(length(warning) != 0) print(warning)
  warning <- ifelse(length(warning) == 0, NA, warning)
  
  ## get predictions for each model in turn ##
  # model 1
  preds <- predict(mod.1, test, type="response", re.form = ~0)
  preds_obj <- prediction(preds, test$phs_01)
  roc_obj <- performance(preds_obj, measure = "tpr", x.measure = "fpr")
  auc_obj <- performance(preds_obj, measure = "auc")
  m1_performance_df <- cbind(get_ROCR_data(roc_obj),
                          get_ROCR_data(auc_obj))
  m1_performance_df$warnings <- warning
  
  # model 2
  preds <- predict(mod.2, test, type="response", re.form = ~0)
  preds_obj <- prediction(preds, test$phs_01)
  roc_obj <- performance(preds_obj, measure = "tpr", x.measure = "fpr")
  auc_obj <- performance(preds_obj, measure = "auc")
  m2_performance_df <- cbind(get_ROCR_data(roc_obj),
                          get_ROCR_data(auc_obj))
  m2_performance_df$warnings <- warning
  
  # model 3
  preds <- predict(mod.3, test, type="response", re.form = ~0)
  preds_obj <- prediction(preds, test$phs_01)
  roc_obj <- performance(preds_obj, measure = "tpr", x.measure = "fpr")
  auc_obj <- performance(preds_obj, measure = "auc")
  m3_performance_df <- cbind(get_ROCR_data(roc_obj),
                          get_ROCR_data(auc_obj))
  m3_performance_df$warnings <- warning
  
  # model 4
  preds <- predict(mod.4, test, type="response", re.form = ~0)
  preds_obj <- prediction(preds, test$phs_01)
  roc_obj <- performance(preds_obj, measure = "tpr", x.measure = "fpr")
  auc_obj <- performance(preds_obj, measure = "auc")
  m4_performance_df <- cbind(get_ROCR_data(roc_obj),
                          get_ROCR_data(auc_obj))
  m4_performance_df$warnings <- warning
  
  # model 5
  preds <- predict(mod.5, test, type="response", re.form = ~0)
  preds_obj <- prediction(preds, test$phs_01)
  roc_obj <- performance(preds_obj, measure = "tpr", x.measure = "fpr")
  auc_obj <- performance(preds_obj, measure = "auc")
  m5_performance_df <- cbind(get_ROCR_data(roc_obj),
                          get_ROCR_data(auc_obj))
  m5_performance_df$warnings <- warning
  
  # model 6
  preds <- predict(mod.6, test, type="response", re.form = ~0)
  preds_obj <- prediction(preds, test$phs_01)
  roc_obj <- performance(preds_obj, measure = "tpr", x.measure = "fpr")
  auc_obj <- performance(preds_obj, measure = "auc")
  m6_performance_df <- cbind(get_ROCR_data(roc_obj),
                          get_ROCR_data(auc_obj))
  m6_performance_df$warnings <- warning
  
  # model 7
  preds <- predict(mod.7, test, type="response", re.form = ~0)
  preds_obj <- prediction(preds, test$phs_01)
  roc_obj <- performance(preds_obj, measure = "tpr", x.measure = "fpr")
  auc_obj <- performance(preds_obj, measure = "auc")
  m7_performance_df <- cbind(get_ROCR_data(roc_obj),
                          get_ROCR_data(auc_obj))
  m7_performance_df$warnings <- warning
  
  # combine performance of each model
  performance_df <- list(m1_performance_df, m2_performance_df,
                         m3_performance_df, m4_performance_df,
                         m5_performance_df, m6_performance_df,
                         m7_performance_df)
  
  return(performance_df)
}
```

# Run bootstrapping to get idea of model performance
```{r, warning = FALSE, message = FALSE}
if (isTRUE(run_bootstrap == 1)){
  # now get a list of dfs and get_performance_df function and get bs results!
  # do this for all models
  m1_performance <- data.frame(True_positive_rate = numeric(), 
                               False_positive_rate = numeric(), 
                               Cutoff = numeric(), 
                               AUC = numeric(), 
                               model = numeric(),
                               warnings = character())
  m2_performance <- data.frame(True_positive_rate = numeric(), 
                               False_positive_rate = numeric(), 
                               Cutoff = numeric(), 
                               AUC = numeric(), 
                               model = numeric(),
                               warnings = character())
  m3_performance <- data.frame(True_positive_rate = numeric(), 
                               False_positive_rate = numeric(), 
                               Cutoff = numeric(), 
                               AUC = numeric(), 
                               model = numeric(),
                               warnings = character())
  m4_performance <- data.frame(True_positive_rate = numeric(), 
                               False_positive_rate = numeric(), 
                               Cutoff = numeric(), 
                               AUC = numeric(), 
                               model = numeric(),
                               warnings = character())
  m5_performance <- data.frame(True_positive_rate = numeric(), 
                               False_positive_rate = numeric(), 
                               Cutoff = numeric(), 
                               AUC = numeric(), 
                               model = numeric(),
                               warnings = character())
  m6_performance <- data.frame(True_positive_rate = numeric(), 
                               False_positive_rate = numeric(), 
                               Cutoff = numeric(), 
                               AUC = numeric(), 
                               model = numeric(),
                               warnings = character())
  m7_performance <- data.frame(True_positive_rate = numeric(), 
                               False_positive_rate = numeric(), 
                               Cutoff = numeric(), 
                               AUC = numeric(), 
                               model = numeric(),
                               warnings = character())
  
  # running bootstrapping on all models to extract CIs for AUCs
  for (i in seq.int(B)) {
        performance <- try(get_performance_df(bs_train[[i]], bs_test[[i]]))
        if (isTRUE(length(performance) > 1)){
          performance[[1]]$model <- i
          performance[[2]]$model <- i
          performance[[3]]$model <- i
          performance[[4]]$model <- i
          performance[[5]]$model <- i
          performance[[6]]$model <- i
          performance[[7]]$model <- i
          # extract data from simulation
          m1_performance <- rbind(m1_performance, performance[[1]])
          m2_performance <- rbind(m2_performance, performance[[2]])
          m3_performance <- rbind(m3_performance, performance[[3]])
          m4_performance <- rbind(m4_performance, performance[[4]])
          m5_performance <- rbind(m5_performance, performance[[5]])
          m6_performance <- rbind(m6_performance, performance[[6]])
          m7_performance <- rbind(m7_performance, performance[[7]])
        }
  }
  
  # save data-frames specific to innocuous analysis
  m1_bs_perf <- m1_performance
  m2_bs_perf <- m2_performance
  m3_bs_perf <- m3_performance
  m4_bs_perf <- m4_performance
  m5_bs_perf <- m5_performance
  m6_bs_perf <- m6_performance
  m7_bs_perf <- m7_performance
  
  write.csv(m1_bs_perf, file = 'm1-bootstrapAUCs.csv', row.names = FALSE)
  write.csv(m2_bs_perf, file = 'm2-bootstrapAUCs.csv', row.names = FALSE)
  write.csv(m3_bs_perf, file = 'm3-bootstrapAUCs.csv', row.names = FALSE)
  write.csv(m4_bs_perf, file = 'm4-bootstrapAUCs.csv', row.names = FALSE)
  write.csv(m5_bs_perf, file = 'm5-bootstrapAUCs.csv', row.names = FALSE)
  write.csv(m6_bs_perf, file = 'm6-bootstrapAUCs.csv', row.names = FALSE)
  write.csv(m7_bs_perf, file = 'm7-bootstrapAUCs.csv', row.names = FALSE)
}
```

