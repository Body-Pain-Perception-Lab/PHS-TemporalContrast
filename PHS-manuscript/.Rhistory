install.packages('car')
knitr::opts_chunk$set(echo = TRUE)
# Load required packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(DHARMa, car, lme4, lmerTest, tidyverse, rcompanion, Rmisc, emmeans)
# read in data
path <- getwd() #make sure set to current folder
tsl_file <- file.path("data", "anonymised_tsl2_208.csv")
# load all data file
if (file.exists(tsl_file)){
tsl2 <- read.csv(tsl_file)
} else {
print('Data file does not exist, check data is in current directory.')
}
calc_z <- function (value) {
z_score <- (value - mean(value))/sd(value)
}
# Select trials for analysis
df1 <- tsl2 %>%
filter(task == "tsl2" & trials_to_keep == 1) %>%
dplyr::rename(subject = exp_id)
# Average TSL2 threshold (i.e. average temperature for the detection of a new thermal change or a painful sensation)
tsl2_threshold <- aggregate(df1$threshold,
by = list(subject = df1$subject,
baseline = df1$baseline,
instruction = df1$instruction),
FUN = mean)
# Count of paradoxical heat sensations per participants (i.e., number of cooling trials perceived as warm, hot or painfully hot)
phs_count <- aggregate(df1$phs, by = list(subject = df1$subject,
baseline = df1$baseline,
instruction = df1$instruction),
FUN = sum)
# Define PHS responders (i.e., 0 = all cooling trials perceived as cold, 1 = at least one cooling trial misperceived as warm, hot or painfully hot) for each condition
phs_01 <- as.numeric(phs_count$x != 0)
# Create new dataframe for further analyses
df <- cbind(tsl2_threshold, phs = phs_count$x, phs_01)
names(df)[names(df) == 'x'] <- 'threshold' # rename column from x to avg_threshold
df$baseline <- as.numeric(df$baseline) # define baseline as double
write.csv(df, file.path("data", "summary_tsl2_208.csv"), row.names = FALSE)
df
# Calculate the total number of PHS responders per condition
df_phs <- aggregate(df$phs_01,
by = list(baseline = df$baseline,
instruction = df$instruction),
FUN = sum)
names(df_phs)[names(df_phs) == 'x'] <- 'N_phs_responders' # rename column
# Calculate the total number of PHS trials per condition
N_phs_trials <- aggregate(df$phs,
by = list(baseline = df$baseline,
instruction = df$instruction),
FUN = sum)
# Create new dataframe
df_phs <- cbind(df_phs, N_phs_trials = N_phs_trials$x)
df_phs
phs_mcnemar_stats <- function(temp1, temp2, instr1, instr2)
{ #Subset data for the two conditions
df1 <- subset(df, baseline == temp1 & instruction == instr1)
df2 <- subset(df, baseline == temp2 & instruction == instr2)
# Extract PHS response vector
phs_df1 <- df1$phs_01
phs_df2 <- df2$phs_01
# Compute contingency table counts
yy <- sum(phs_df1 == 1 & phs_df2 == 1)
yn <- sum(phs_df1 == 1 & phs_df2 == 0)
ny <- sum(phs_df1 == 0 & phs_df2 == 1)
nn <- sum(phs_df1 == 0 & phs_df2 == 0)
# Create a contingency table
phs_comparison <- matrix(c(yy, yn, ny, nn),
nrow = 2,
dimnames = list("temp1" = c("phs_yes", "phs_no"),
"temp2" = c("phs_yes", "phs_no")))
# Perform McNemar's test and compute effect size (Cohen's g)
phs_stats <- mcnemar.test(phs_comparison)
phs_cohenG <- cohenG(phs_comparison) # OR = odds ratio,  g = Cohenâ€™s g (effect size estimates)
# Return results
results=list(phs_stats,phs_cohenG)
return(results)
}
# Innocuous PHS comparisons
res1 <- phs_mcnemar_stats(38,32,"detect","detect")
res2 <- phs_mcnemar_stats(44,32,"detect","detect")
res3 <- phs_mcnemar_stats(44,38,"detect","detect")
# Noxious PHS comparisons
res4 <- phs_mcnemar_stats(38,32,"pain", "pain")
res5 <- phs_mcnemar_stats(44,32,"pain", "pain")
res6 <- phs_mcnemar_stats(44,38,"pain", "pain")
# Innocuous vs. noxious PHS comparisons
res7 <- phs_mcnemar_stats(32,32,"detect","pain")
res8 <- phs_mcnemar_stats(38,38,"detect","pain")
res9 <- phs_mcnemar_stats(44,44,"detect","pain")
# Combine the results into a data frame
results <- data.frame(
Comparison = c("Innoc 32 vs 38", "Innoc 32 vs 44", "Innoc 38 vs 44",
"Nox 32 vs 38", "Nox 32 vs 44", "Nox 38 vs 44",
"Innoc vs Nox (32)", "Innoc vs Nox (38)", "Innoc vs Nox (44)"),
McNemar_chi_squared = round(c(res1[[1]]$statistic, res2[[1]]$statistic, res3[[1]]$statistic,
res4[[1]]$statistic, res5[[1]]$statistic, res6[[1]]$statistic,
res7[[1]]$statistic, res8[[1]]$statistic, res9[[1]]$statistic), 3),
McNemar_p_value = round(c(res1[[1]]$p.value, res2[[1]]$p.value, res3[[1]]$p.value,
res4[[1]]$p.value, res5[[1]]$p.value, res6[[1]]$p.value,
res7[[1]]$p.value, res8[[1]]$p.value, res9[[1]]$p.value), 3),
OddsR = round(c(res1[[2]]$Global.statistics$OR, res2[[2]]$Global.statistics$OR,
res3[[2]]$Global.statistics$OR,res4[[2]]$Global.statistics$OR,
res5[[2]]$Global.statistics$OR, res6[[2]]$Global.statistics$OR,
res7[[2]]$Global.statistics$OR, res8[[2]]$Global.statistics$OR,
res9[[2]]$Global.statistics$OR), 3)
)
# Print the table of results
print(results)
#df$phs <- as.numeric(as.character(df$phs)) #double check it is numeric
df1$baseline <- as.factor(df1$baseline)
df1$trial_z <- calc_z(df1$trial) #trial zscore
df1$subject <- as.factor(df1$subject)
df1$instruction <- as.factor(df1$instruction)
# phs by contrast and instruction
m.phs1 <- lme4::glmer(as.factor(phs) ~ baseline * instruction + (1|subject),
data = df1,
family = 'binomial')
# phs by contrast and instruction
m.phs1 <- lme4::glmer(as.factor(phs) ~ baseline * instruction + (1|subject),
data = df1,
family = 'binomial')
install.packages("lme4", type = "source")
install.packages("lme4", type = "source")
library(lme4)
knitr::opts_chunk$set(echo = TRUE)
# Load required packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(DHARMa, car, lme4, lmerTest, tidyverse, rcompanion, Rmisc, emmeans)
# phs by contrast and instruction
m.phs1 <- lme4::glmer(as.factor(phs) ~ baseline * instruction + (1|subject),
data = df1,
family = 'binomial')
#family = poisson(link = "log"))
summary(m.phs1)
or.1 <- tidy(m.phs1, conf.int=TRUE,
exponentiate=TRUE, effects="fixed")
or.1
library(tidyverse)
or.1 <- tidy(m.phs1, conf.int=TRUE,
exponentiate=TRUE, effects="fixed")
install.packages('broom')
library(broom)
knitr::opts_chunk$set(echo = TRUE)
# Load required packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(DHARMa, broom, car, lme4, lmerTest, tidyverse, rcompanion, Rmisc, emmeans)
or.1 <- tidy(m.phs1, conf.int=TRUE,
exponentiate=TRUE, effects="fixed")
library(broom.mixed)
knitr::opts_chunk$set(echo = TRUE)
# Load required packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(DHARMa, broom.mixed, car, lme4, lmerTest, tidyverse, rcompanion, Rmisc, emmeans)
or.1 <- tidy(m.phs1, conf.int=TRUE,
exponentiate=TRUE, effects="fixed")
or.1
# phs by contrast and instruction including trial
# trial makes very little difference here
m.phs2 <- lme4::glmer(as.factor(phs) ~ baseline * instruction + trial_z +
(1|subject),
data = df1,
family = 'binomial')
# phs by contrast and instruction including trial
# trial makes very little difference here
m.phs2 <- lme4::glmer(as.factor(phs) ~ baseline * instruction + trial_z +
(1|subject),
data = df1,
family = 'binomial')
#family = poisson(link = "log"))
summary(m.phs2)
or.2 <- tidy(m.phs2, conf.int=TRUE,
exponentiate=TRUE, effects="fixed")
or.2
# model comparison
anova(m.phs1, m.phs2)
anova(m.phs1, type = 2)
anova(m.phs1)
ftest <- anova(m.phs1)
ftest
ftest <- anova(m.phs1, test = 'Chisq')
ftest
# anova model - simplfy and see where this leads me
m.phs3 <- lme4::glm(as.factor(phs) ~ baseline * instruction,
data = df1,
family = 'binomial')
# anova model - simplfy and see where this leads me
m.phs3 <- glm(as.factor(phs) ~ baseline * instruction,
data = df1,
family = 'binomial')
ftest <- anova(m.phs3)
ftest
ftest <- anova(m.phs3, type = 2)
ftest
#family = poisson(link = "log"))
summary(m.phs3)
# anova model - simplfy and see where this leads me
m.phs3 <- glm(as.factor(phs) ~ baseline * instruction,
data = df1,
family = 'binomial')
#family = poisson(link = "log"))
summary(m.phs3)
ftest <- anova(m.phs3, type = 2)
ftest
ftest <- anova(m.phs3, test = 'Chisq')
ftest
phoc <- emmeans(m.phs3, ~ baseline * instruction)
phoc
pw <- pairs(phoc, simple = 'baseline')
pw
View(pw)
# this is getting unweildy, simplify (when you have the time)
knitr::opts_chunk$set(echo = TRUE)
# check for pacman package and install if not found
if (!require("pacman")) install.packages("pacman")
pacman::p_load(boot, broom.mixed, caret, cluster, factoextra, ggeffects,
gghalves, ggpol, ggpubr, groupdata2, lme4, lmerTest,
RColorBrewer, rcompanion,
reshape2, Rmisc, ROCR, rsample, tidyverse, wesanderson
)
# read in data
tsl_file <- file.path("data", "anonymised_tsl2_208.csv")
sum_file <- file.path("data", "summary_tsl2_208.csv")
# load all data file
if (file.exists(tsl_file)){
tsl2 <- read.csv(tsl_file)
tsl2 <- tsl2 %>%
filter(trials_to_keep == 1)
} else {
print('Data file does not exist, check data is in current directory.')
}
# load data summary file
if (file.exists(sum_file)){
df <- read.csv(sum_file)
} else {
print('Data file does not exist, check data is in current directory if not, run tsl2_phs_q1.Rmd first')
}
calc_z <- function (value) {
z_score <- (value - mean(value))/sd(value)
}
# extract detection trial only
phs_detect <- df %>%
filter(instruction == 'detect') %>%
dplyr::rename(phs_detect_bin = phs_01,
phs_detect = phs)
# extract pain trials only and then add phs_detect to these
phs_pain <- df %>%
filter(instruction == 'pain') %>%
dplyr::rename(phs_pain_bin = phs_01,
phs_pain = phs)
# add detection columns to PHS pain
phs_pain1 <- phs_detect %>%
left_join(phs_pain, phs_detect, by = c('subject','baseline')) %>%
select(-c(instruction.y,threshold.y)) %>%
dplyr::rename(threshold = threshold.x,
instruction = instruction.x)
# create large data_frame where only phs during detection is considered
df_detect <- phs_pain1 %>%
select(-c(phs_pain, phs_pain_bin))
df_detect <- rbind(df_detect, phs_detect)
N = 208
# detect by phs
tsl2_mean <- aggregate(threshold~instruction*baseline*phs_01, mean, data = df)
tsl2_median <- aggregate(threshold~instruction*baseline*phs_01, median, data = df)
tsl2_sd <- aggregate(threshold~instruction*baseline*phs_01, sd, data = df)
tsl2_stderr <- tsl2_sd$threshold/sqrt(N)# number of participants
tsl2_ci <- aggregate(threshold~instruction*baseline*phs_01, CI, data = df)
# data frame
tsl2_stats <- data.frame(baseline = tsl2_mean$baseline, phs_detect = tsl2_mean$phs_01,
mean = tsl2_mean$threshold, median = tsl2_median$threshold,
sd = tsl2_sd$threshold,
stderr = tsl2_stderr, tsl2_ci$threshold,
instruction = tsl2_median$instruction)
# rename CI labels
names(tsl2_stats)[names(tsl2_stats) == 'upper'] <- 'ci_upper'
names(tsl2_stats)[names(tsl2_stats) == 'mean.1'] <- 'ci_mean'
names(tsl2_stats)[names(tsl2_stats) == 'lower'] <- 'ci_lower'
# calculate ci
tsl2_stats$ci <- tsl2_stats$ci_upper - tsl2_stats$ci_lower
# all thresholds (regardless of phs)
tsl2_mean <- aggregate(threshold~baseline*instruction, mean, data = df)
tsl2_median <- aggregate(threshold~baseline*instruction, median, data = df)
tsl2_sd <- aggregate(threshold~baseline*instruction, sd, data = df)
tsl2_stderr <- tsl2_sd$threshold/sqrt(N)# number of participants
tsl2_ci <- aggregate(threshold~baseline*instruction, CI, data = df)
# data frame
tsl2_all_stats <- data.frame(baseline = tsl2_mean$baseline, instruction = tsl2_mean$instruction,
mean = tsl2_mean$threshold, median = tsl2_median$threshold,
sd = tsl2_sd$threshold, stderr = tsl2_stderr, tsl2_ci$threshold)
# rename CI labels
names(tsl2_all_stats)[names(tsl2_all_stats) == 'upper'] <- 'ci_upper'
names(tsl2_all_stats)[names(tsl2_all_stats) == 'mean.1'] <- 'ci_mean'
names(tsl2_all_stats)[names(tsl2_all_stats) == 'lower'] <- 'ci_lower'
# calculate ci
tsl2_all_stats$ci <- tsl2_all_stats$ci_upper - tsl2_all_stats$ci_lower
write.csv(tsl2_stats, file.path("results", "phs_threshold_stats.csv"), row.names = FALSE)
tsl2_stats
write.csv(tsl2_all_stats, file.path("results", "all_threshold_stats.csv"), row.names = FALSE)
tsl2_all_stats
tsl2$trial_z <- calc_z(tsl2$trial)
# innocuous TSL thresholds
m.detect <- lmer(threshold ~ as.factor(baseline) + trial_z + (1|exp_id),
data = tsl2 %>% filter(instruction == 'detect'))
summary(m.detect, ddf = "Satterthwaite")
# noxious TSL thresholds
m.pain <- lmer(threshold ~ as.factor(baseline) + trial_z + (1|exp_id),
data = tsl2 %>% filter(instruction == 'pain'))
summary(m.pain, ddf = "Satterthwaite")
rm(list=ls())
# Select trials for analysis
df1 <- tsl2 %>%
filter(task == "tsl2" & trials_to_keep == 1) %>%
dplyr::rename(subject = exp_id)
# read in data
path <- getwd() #make sure set to current folder
tsl_file <- file.path("data", "anonymised_tsl2_208.csv")
# load all data file
if (file.exists(tsl_file)){
tsl2 <- read.csv(tsl_file)
} else {
print('Data file does not exist, check data is in current directory.')
}
# Select trials for analysis
df1 <- tsl2 %>%
filter(task == "tsl2" & trials_to_keep == 1) %>%
dplyr::rename(subject = exp_id)
# Average TSL2 threshold (i.e. average temperature for the detection of a new thermal change or a painful sensation)
tsl2_threshold <- aggregate(df1$threshold,
by = list(subject = df1$subject,
baseline = df1$baseline,
instruction = df1$instruction),
FUN = mean)
# Count of paradoxical heat sensations per participants (i.e., number of cooling trials perceived as warm, hot or painfully hot)
phs_count <- aggregate(df1$phs, by = list(subject = df1$subject,
baseline = df1$baseline,
instruction = df1$instruction),
FUN = sum)
# Define PHS responders (i.e., 0 = all cooling trials perceived as cold, 1 = at least one cooling trial misperceived as warm, hot or painfully hot) for each condition
phs_01 <- as.numeric(phs_count$x != 0)
# Create new dataframe for further analyses
df <- cbind(tsl2_threshold, phs = phs_count$x, phs_01)
names(df)[names(df) == 'x'] <- 'threshold' # rename column from x to avg_threshold
df$baseline <- as.numeric(df$baseline) # define baseline as double
write.csv(df, file.path("data", "summary_tsl2_208.csv"), row.names = FALSE)
df
# Calculate the total number of PHS responders per condition
df_phs <- aggregate(df$phs_01,
by = list(baseline = df$baseline,
instruction = df$instruction),
FUN = sum)
names(df_phs)[names(df_phs) == 'x'] <- 'N_phs_responders' # rename column
# Calculate the total number of PHS trials per condition
N_phs_trials <- aggregate(df$phs,
by = list(baseline = df$baseline,
instruction = df$instruction),
FUN = sum)
# Create new dataframe
df_phs <- cbind(df_phs, N_phs_trials = N_phs_trials$x)
df_phs
phs_mcnemar_stats <- function(temp1, temp2, instr1, instr2)
{ #Subset data for the two conditions
df1 <- subset(df, baseline == temp1 & instruction == instr1)
df2 <- subset(df, baseline == temp2 & instruction == instr2)
# Extract PHS response vector
phs_df1 <- df1$phs_01
phs_df2 <- df2$phs_01
# Compute contingency table counts
yy <- sum(phs_df1 == 1 & phs_df2 == 1)
yn <- sum(phs_df1 == 1 & phs_df2 == 0)
ny <- sum(phs_df1 == 0 & phs_df2 == 1)
nn <- sum(phs_df1 == 0 & phs_df2 == 0)
# Create a contingency table
phs_comparison <- matrix(c(yy, yn, ny, nn),
nrow = 2,
dimnames = list("temp1" = c("phs_yes", "phs_no"),
"temp2" = c("phs_yes", "phs_no")))
# Perform McNemar's test and compute effect size (Cohen's g)
phs_stats <- mcnemar.test(phs_comparison)
phs_cohenG <- cohenG(phs_comparison) # OR = odds ratio,  g = Cohenâ€™s g (effect size estimates)
# Return results
results=list(phs_stats,phs_cohenG)
return(results)
}
# Innocuous PHS comparisons
res1 <- phs_mcnemar_stats(38,32,"detect","detect")
res1
# Innocuous PHS comparisons
res1 <- phs_mcnemar_stats(38,32,"detect","detect")
res2 <- phs_mcnemar_stats(44,32,"detect","detect")
res3 <- phs_mcnemar_stats(44,38,"detect","detect")
# Noxious PHS comparisons
res4 <- phs_mcnemar_stats(38,32,"pain", "pain")
res5 <- phs_mcnemar_stats(44,32,"pain", "pain")
res6 <- phs_mcnemar_stats(44,38,"pain", "pain")
# Innocuous vs. noxious PHS comparisons
res7 <- phs_mcnemar_stats(32,32,"detect","pain")
res8 <- phs_mcnemar_stats(38,38,"detect","pain")
res9 <- phs_mcnemar_stats(44,44,"detect","pain")
# Combine the results into a data frame
results <- data.frame(
Comparison = c("Innoc 32 vs 38", "Innoc 32 vs 44", "Innoc 38 vs 44",
"Nox 32 vs 38", "Nox 32 vs 44", "Nox 38 vs 44",
"Innoc vs Nox (32)", "Innoc vs Nox (38)", "Innoc vs Nox (44)"),
McNemar_chi_squared = round(c(res1[[1]]$statistic, res2[[1]]$statistic, res3[[1]]$statistic,
res4[[1]]$statistic, res5[[1]]$statistic, res6[[1]]$statistic,
res7[[1]]$statistic, res8[[1]]$statistic, res9[[1]]$statistic), 3),
McNemar_p_value = round(c(res1[[1]]$p.value, res2[[1]]$p.value, res3[[1]]$p.value,
res4[[1]]$p.value, res5[[1]]$p.value, res6[[1]]$p.value,
res7[[1]]$p.value, res8[[1]]$p.value, res9[[1]]$p.value), 3),
OddsR = round(c(res1[[2]]$Global.statistics$g, res2[[2]]$Global.statistics$g,
res3[[2]]$Global.statistics$g,res4[[2]]$Global.statistics$g,
res5[[2]]$Global.statistics$g, res6[[2]]$Global.statistics$g,
res7[[2]]$Global.statistics$g, res8[[2]]$Global.statistics$g,
res9[[2]]$Global.statistics$g), 3)
)
# Print the table of results
print(results)
# Innocuous PHS comparisons
res1 <- phs_mcnemar_stats(38,32,"detect","detect")
res2 <- phs_mcnemar_stats(44,32,"detect","detect")
res3 <- phs_mcnemar_stats(44,38,"detect","detect")
# Noxious PHS comparisons
res4 <- phs_mcnemar_stats(38,32,"pain", "pain")
res5 <- phs_mcnemar_stats(44,32,"pain", "pain")
res6 <- phs_mcnemar_stats(44,38,"pain", "pain")
# Innocuous vs. noxious PHS comparisons
res7 <- phs_mcnemar_stats(32,32,"detect","pain")
res8 <- phs_mcnemar_stats(38,38,"detect","pain")
res9 <- phs_mcnemar_stats(44,44,"detect","pain")
# Combine the results into a data frame
results <- data.frame(
Comparison = c("Innoc 32 vs 38", "Innoc 32 vs 44", "Innoc 38 vs 44",
"Nox 32 vs 38", "Nox 32 vs 44", "Nox 38 vs 44",
"Innoc vs Nox (32)", "Innoc vs Nox (38)", "Innoc vs Nox (44)"),
McNemar_chi_squared = round(c(res1[[1]]$statistic, res2[[1]]$statistic, res3[[1]]$statistic,
res4[[1]]$statistic, res5[[1]]$statistic, res6[[1]]$statistic,
res7[[1]]$statistic, res8[[1]]$statistic, res9[[1]]$statistic), 3),
McNemar_p_value = round(c(res1[[1]]$p.value, res2[[1]]$p.value, res3[[1]]$p.value,
res4[[1]]$p.value, res5[[1]]$p.value, res6[[1]]$p.value,
res7[[1]]$p.value, res8[[1]]$p.value, res9[[1]]$p.value), 3),
CohensG = round(c(res1[[2]]$Global.statistics$g, res2[[2]]$Global.statistics$g,
res3[[2]]$Global.statistics$g,res4[[2]]$Global.statistics$g,
res5[[2]]$Global.statistics$g, res6[[2]]$Global.statistics$g,
res7[[2]]$Global.statistics$g, res8[[2]]$Global.statistics$g,
res9[[2]]$Global.statistics$g), 3)
)
# Print the table of results
print(results)
